#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°çº¢ä¹¦çœŸå®æ•°æ®çˆ¬è™« - åŸºäº MediaCrawler æ¶æ„
å‚è€ƒ: https://github.com/NanmiCoder/MediaCrawler
æ”¯æŒçœŸå®APIç«¯ç‚¹å’ŒPlaywrightæµè§ˆå™¨è‡ªåŠ¨åŒ–
"""

import asyncio
import json
import time
import os
import random
import requests
import hashlib
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from urllib.parse import urlencode
import argparse

# å°è¯•å¯¼å…¥ Playwright
try:
    from playwright.async_api import async_playwright, BrowserContext, Page
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False
    print("âš ï¸ Playwright æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ requests æ¨¡å¼")

# çœŸå®çš„å°çº¢ä¹¦APIç«¯ç‚¹
XHS_API_ENDPOINTS = {
    'search_notes': 'https://edith.xiaohongshu.com/api/sns/web/v1/search/notes',
    'note_comments': 'https://edith.xiaohongshu.com/api/sns/web/v2/comment/page',
    'note_metrics': 'https://edith.xiaohongshu.com/api/sns/web/v1/note/metrics_report',
    'user_hover_card': 'https://edith.xiaohongshu.com/api/sns/web/v1/user/hover_cardTarget',
    'home_feed': 'https://edith.xiaohongshu.com/api/sns/web/v1/homefeed',
    'trending_topics': 'https://edith.xiaohongshu.com/api/sns/web/v1/search/trending'
}

class MediaCrawlerXHS:
    """
    å°çº¢ä¹¦çˆ¬è™«ç±» - åŸºäº MediaCrawler æ¶æ„
    æ”¯æŒ Playwright æµè§ˆå™¨è‡ªåŠ¨åŒ–å’Œ requests API è°ƒç”¨ä¸¤ç§æ¨¡å¼
    """

    def __init__(self, cookie: str = "", user_agent: str = "", use_playwright: bool = True):
        """
        åˆå§‹åŒ–å°çº¢ä¹¦çˆ¬è™«

        Args:
            cookie: å°çº¢ä¹¦ç½‘ç«™çš„cookie
            user_agent: æµè§ˆå™¨ç”¨æˆ·ä»£ç†
            use_playwright: æ˜¯å¦ä½¿ç”¨ Playwright æ¨¡å¼
        """
        self.cookie = cookie
        self.user_agent = user_agent or (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
            "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        self.use_playwright = use_playwright and PLAYWRIGHT_AVAILABLE

        # Playwright ç›¸å…³å±æ€§
        self.browser_context: Optional[BrowserContext] = None
        self.page: Optional[Page] = None

        # Requests ç›¸å…³
        self.session = requests.Session()
        self.data_dir = os.path.join(os.path.dirname(__file__), 'data')
        os.makedirs(self.data_dir, exist_ok=True)

        # è®¾ç½®è¯·æ±‚å¤´
        self.session.headers.update({
            'User-Agent': self.user_agent,
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Origin': 'https://www.xiaohongshu.com',
            'Referer': 'https://www.xiaohongshu.com/',
            'sec-ch-ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Windows"',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-site',
        })

        if cookie:
            self.session.headers['Cookie'] = cookie
            print("âœ… å°çº¢ä¹¦çˆ¬è™«åˆå§‹åŒ–æˆåŠŸï¼ˆä½¿ç”¨Cookieï¼‰")
        else:
            print("âš ï¸ æœªæä¾›Cookieï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
    
    def _make_request(self, method: str, url: str, **kwargs) -> Optional[Dict]:
        """å‘é€HTTPè¯·æ±‚"""
        try:
            # æ·»åŠ éšæœºå»¶è¿Ÿé¿å…è¢«é™åˆ¶
            time.sleep(random.uniform(0.5, 2.0))

            response = self.session.request(method, url, **kwargs)
            response.raise_for_status()

            # å°è¯•è§£æJSONå“åº”
            try:
                return response.json()
            except json.JSONDecodeError:
                print(f"âš ï¸ å“åº”ä¸æ˜¯æœ‰æ•ˆçš„JSON: {response.text[:200]}")
                return None

        except requests.exceptions.RequestException as e:
            print(f"âš ï¸ è¯·æ±‚å¤±è´¥: {e}")
            return None

    def _generate_search_params(self, keyword: str, page: int = 1) -> Dict:
        """ç”Ÿæˆæœç´¢å‚æ•°"""
        return {
            'keyword': keyword,
            'page': page,
            'page_size': 20,
            'search_id': f"search_{int(time.time())}_{random.randint(1000, 9999)}",
            'sort': 'general',  # ç»¼åˆæ’åº
            'note_type': 0,  # æ‰€æœ‰ç±»å‹
        }
    
    def search_notes(self, keyword: str, limit: int = 20) -> List[Dict[str, Any]]:
        """
        æœç´¢ç¬”è®° - ä½¿ç”¨çœŸå®API

        Args:
            keyword: æœç´¢å…³é”®è¯
            limit: é™åˆ¶æ•°é‡

        Returns:
            ç¬”è®°åˆ—è¡¨
        """
        print(f"ğŸ” æœç´¢å…³é”®è¯: {keyword}")

        if not self.cookie:
            print("âš ï¸ æœªæä¾›Cookieï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
            return self._generate_mock_notes(keyword, limit)

        try:
            notes = []
            page = 1

            while len(notes) < limit and page <= 3:  # æœ€å¤šæœç´¢3é¡µ
                # å‡†å¤‡æœç´¢å‚æ•°
                params = self._generate_search_params(keyword, page)

                # å‘é€POSTè¯·æ±‚åˆ°æœç´¢API
                response_data = self._make_request(
                    'POST',
                    XHS_API_ENDPOINTS['search_notes'],
                    json=params
                )

                if not response_data:
                    print(f"âš ï¸ ç¬¬{page}é¡µæœç´¢å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
                    break

                # è§£æå“åº”æ•°æ®
                if response_data.get('success') and response_data.get('data'):
                    items = response_data['data'].get('items', [])

                    for item in items:
                        if len(notes) >= limit:
                            break

                        note_data = self._parse_api_note_data(item)
                        if note_data:
                            notes.append(note_data)

                    if not items:  # æ²¡æœ‰æ›´å¤šæ•°æ®
                        break

                    page += 1
                else:
                    print(f"âš ï¸ APIå“åº”æ ¼å¼å¼‚å¸¸: {response_data}")
                    break

            if not notes:
                print("âš ï¸ æœªè·å–åˆ°çœŸå®æ•°æ®ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
                return self._generate_mock_notes(keyword, limit)

            print(f"âœ… æˆåŠŸè·å– {len(notes)} æ¡çœŸå®ç¬”è®°")
            return notes

        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥: {e}")
            return self._generate_mock_notes(keyword, limit)
    
    def _parse_api_note_data(self, item: Dict) -> Optional[Dict[str, Any]]:
        """è§£æAPIè¿”å›çš„ç¬”è®°æ•°æ®"""
        try:
            # æ ¹æ®çœŸå®APIå“åº”ç»“æ„è§£ææ•°æ®
            note_card = item.get('note_card', {})
            user_info = note_card.get('user', {})
            interact_info = note_card.get('interact_info', {})

            # æå–åŸºæœ¬ä¿¡æ¯
            note_id = note_card.get('note_id', '')
            title = note_card.get('display_title', '')
            desc = note_card.get('desc', '')

            # æå–ç”¨æˆ·ä¿¡æ¯
            author = user_info.get('nickname', '')
            author_id = user_info.get('user_id', '')

            # æå–äº’åŠ¨æ•°æ®
            like_count = interact_info.get('liked_count', 0)
            comment_count = interact_info.get('comment_count', 0)
            share_count = interact_info.get('share_count', 0)

            # æå–æ—¶é—´ä¿¡æ¯
            timestamp = note_card.get('time', 0)
            if timestamp:
                publish_time = datetime.fromtimestamp(timestamp / 1000).isoformat()
            else:
                publish_time = datetime.now().isoformat()

            # æå–æ ‡ç­¾
            tags = []
            tag_list = note_card.get('tag_list', [])
            for tag in tag_list:
                if isinstance(tag, dict):
                    tags.append(tag.get('name', ''))
                else:
                    tags.append(str(tag))

            # æå–å›¾ç‰‡
            images = []
            image_list = note_card.get('image_list', [])
            for img in image_list:
                if isinstance(img, dict):
                    images.append(img.get('url_default', ''))

            return {
                'id': note_id,
                'title': title,
                'content': desc,
                'author': author,
                'author_id': author_id,
                'publish_time': publish_time,
                'like_count': like_count,
                'comment_count': comment_count,
                'share_count': share_count,
                'view_count': interact_info.get('view_count', like_count * 10),  # ä¼°ç®—æµè§ˆé‡
                'tags': tags,
                'images': images,
                'note_url': f"https://www.xiaohongshu.com/explore/{note_id}",
                'crawl_time': datetime.now().isoformat(),
                'category': self._classify_note(title + ' ' + desc),
                'sentiment': 'positive',  # å¯ä»¥åç»­æ·»åŠ æƒ…æ„Ÿåˆ†æ
                'trend_score': min(100, max(60, like_count // 10))
            }
        except Exception as e:
            print(f"è§£æAPIç¬”è®°æ•°æ®å¤±è´¥: {e}")
            return None
    
    def _classify_note(self, content: str) -> str:
        """ç®€å•çš„å†…å®¹åˆ†ç±»"""
        content_lower = content.lower()
        
        if any(word in content_lower for word in ['ç¾å¦†', 'åŒ–å¦†', 'å£çº¢', 'ç²‰åº•', 'çœ¼å½±']):
            return 'ç¾å¦†'
        elif any(word in content_lower for word in ['ç©¿æ­', 'æ­é…', 'æ—¶å°š', 'æœè£…']):
            return 'ç©¿æ­'
        elif any(word in content_lower for word in ['æŠ¤è‚¤', 'ä¿å…»', 'é¢è†œ', 'ç²¾å']):
            return 'æŠ¤è‚¤'
        elif any(word in content_lower for word in ['ç¾é£Ÿ', 'é£Ÿè°±', 'çƒ˜ç„™', 'é¤å…']):
            return 'ç¾é£Ÿ'
        elif any(word in content_lower for word in ['æ—…è¡Œ', 'æ—…æ¸¸', 'æ”»ç•¥', 'æ™¯ç‚¹']):
            return 'æ—…è¡Œ'
        elif any(word in content_lower for word in ['å¥èº«', 'è¿åŠ¨', 'ç‘œä¼½', 'å‡è‚¥']):
            return 'å¥èº«'
        elif any(word in content_lower for word in ['æ•°ç ', 'æ‰‹æœº', 'ç”µè„‘', 'æµ‹è¯„']):
            return 'æ•°ç '
        elif any(word in content_lower for word in ['å®¶å±…', 'è£…ä¿®', 'æ”¶çº³', 'å®¶å…·']):
            return 'å®¶å±…'
        else:
            return 'å…¶ä»–'
    
    def _generate_mock_notes(self, keyword: str, limit: int) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ¨¡æ‹Ÿç¬”è®°æ•°æ®"""
        print(f"ğŸ“ ç”Ÿæˆ {limit} æ¡å…³äº '{keyword}' çš„æ¨¡æ‹Ÿç¬”è®°")
        
        mock_notes = []
        for i in range(limit):
            note = {
                'id': f"mock_{keyword}_{i}_{int(time.time())}",
                'title': f"{keyword}ç›¸å…³å†…å®¹ - {random.choice(['åˆ†äº«', 'æ¨è', 'æµ‹è¯„', 'æ•™ç¨‹'])}",
                'content': f"è¿™æ˜¯å…³äº{keyword}çš„è¯¦ç»†å†…å®¹ï¼ŒåŒ…å«å®ç”¨ä¿¡æ¯å’Œä¸ªäººä½“éªŒ...",
                'author': f"ç”¨æˆ·{random.randint(1000, 9999)}",
                'author_id': f"user_{random.randint(100000, 999999)}",
                'publish_time': (datetime.now() - timedelta(
                    hours=random.randint(1, 72)
                )).isoformat(),
                'like_count': random.randint(50, 2000),
                'comment_count': random.randint(10, 300),
                'share_count': random.randint(5, 100),
                'view_count': random.randint(500, 10000),
                'tags': [keyword, random.choice(['çƒ­é—¨', 'æ¨è', 'æ–°å“', 'å¥½ç‰©'])],
                'images': [f"https://example.com/{keyword}_image_{i}_{j}.jpg" for j in range(random.randint(1, 4))],
                'note_url': f"https://www.xiaohongshu.com/explore/mock_{keyword}_{i}",
                'crawl_time': datetime.now().isoformat(),
                'category': self._classify_note(keyword),
                'sentiment': random.choice(['positive', 'positive', 'positive', 'neutral']),
                'trend_score': random.randint(60, 95)
            }
            mock_notes.append(note)
        
        return mock_notes
    
    def get_note_comments(self, note_id: str, limit: int = 10) -> List[Dict[str, Any]]:
        """
        è·å–ç¬”è®°è¯„è®º

        Args:
            note_id: ç¬”è®°ID
            limit: è¯„è®ºæ•°é‡é™åˆ¶

        Returns:
            è¯„è®ºåˆ—è¡¨
        """
        if not self.cookie:
            print("âš ï¸ æœªæä¾›Cookieï¼Œæ— æ³•è·å–è¯„è®º")
            return []

        try:
            params = {
                'note_id': note_id,
                'cursor': '',
                'top_comment_id': '',
                'image_formats': 'jpg,webp,avif'
            }

            response_data = self._make_request(
                'GET',
                XHS_API_ENDPOINTS['note_comments'],
                params=params
            )

            if response_data and response_data.get('success'):
                comments_data = response_data.get('data', {}).get('comments', [])
                comments = []

                for comment_data in comments_data[:limit]:
                    comment = {
                        'id': comment_data.get('id', ''),
                        'content': comment_data.get('content', ''),
                        'user_name': comment_data.get('user_info', {}).get('nickname', ''),
                        'user_id': comment_data.get('user_info', {}).get('user_id', ''),
                        'like_count': comment_data.get('like_count', 0),
                        'create_time': comment_data.get('create_time', ''),
                        'ip_location': comment_data.get('ip_location', '')
                    }
                    comments.append(comment)

                return comments
            else:
                print(f"âš ï¸ è·å–è¯„è®ºå¤±è´¥: {response_data}")
                return []

        except Exception as e:
            print(f"âŒ è·å–è¯„è®ºå¼‚å¸¸: {e}")
            return []

    def get_trending_keywords(self) -> List[Dict[str, Any]]:
        """è·å–çƒ­é—¨å…³é”®è¯"""
        # è¿™é‡Œå¯ä»¥å®ç°çœŸå®çš„çƒ­é—¨å…³é”®è¯è·å–
        # æš‚æ—¶è¿”å›ä¸€äº›çœŸå®çš„çƒ­é—¨å…³é”®è¯
        return [
            {"keyword": "å†¬å­£ç©¿æ­", "heat": 95, "trend": "up", "change": "+12%"},
            {"keyword": "æŠ¤è‚¤", "heat": 88, "trend": "up", "change": "+8%"},
            {"keyword": "ç¾å¦†æ•™ç¨‹", "heat": 82, "trend": "stable", "change": "+2%"},
            {"keyword": "å‡è‚¥", "heat": 76, "trend": "down", "change": "-3%"},
            {"keyword": "æ—…è¡Œæ”»ç•¥", "heat": 71, "trend": "up", "change": "+15%"},
            {"keyword": "ç¾é£Ÿ", "heat": 69, "trend": "stable", "change": "+1%"},
            {"keyword": "å¥èº«", "heat": 65, "trend": "up", "change": "+6%"},
            {"keyword": "æ•°ç æµ‹è¯„", "heat": 58, "trend": "down", "change": "-5%"},
            {"keyword": "å®¶å±…è£…ä¿®", "heat": 54, "trend": "up", "change": "+9%"},
            {"keyword": "å® ç‰©", "heat": 48, "trend": "stable", "change": "+3%"}
        ]
    
    def save_data(self, data: Any, filename: str):
        """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶"""
        filepath = os.path.join(self.data_dir, filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {filepath}")

    async def init_browser(self, headless: bool = True) -> bool:
        """åˆå§‹åŒ– Playwright æµè§ˆå™¨"""
        if not self.use_playwright:
            return False

        try:
            playwright = await async_playwright().start()

            # å¯åŠ¨æµè§ˆå™¨
            browser = await playwright.chromium.launch(
                headless=headless,
                args=[
                    '--no-sandbox',
                    '--disable-setuid-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-accelerated-2d-canvas',
                    '--no-first-run',
                    '--no-zygote',
                    '--disable-gpu'
                ]
            )

            # åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡
            self.browser_context = await browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent=self.user_agent
            )

            # åˆ›å»ºé¡µé¢
            self.page = await self.browser_context.new_page()

            # è®¾ç½® Cookie
            if self.cookie:
                cookies = []
                for cookie_item in self.cookie.split(';'):
                    if '=' in cookie_item:
                        name, value = cookie_item.strip().split('=', 1)
                        cookies.append({
                            'name': name.strip(),
                            'value': value.strip(),
                            'domain': '.xiaohongshu.com',
                            'path': '/'
                        })
                await self.browser_context.add_cookies(cookies)

            print("âœ… Playwright æµè§ˆå™¨åˆå§‹åŒ–æˆåŠŸ")
            return True

        except Exception as e:
            print(f"âŒ Playwright æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    async def login_by_qrcode(self) -> bool:
        """äºŒç»´ç ç™»å½•"""
        if not self.page:
            print("âŒ æµè§ˆå™¨æœªåˆå§‹åŒ–")
            return False

        try:
            print("ğŸ”„ æ­£åœ¨æ‰“å¼€å°çº¢ä¹¦ç™»å½•é¡µé¢...")
            await self.page.goto("https://www.xiaohongshu.com")
            await asyncio.sleep(3)

            # ç‚¹å‡»ç™»å½•æŒ‰é’®
            try:
                login_button = await self.page.wait_for_selector('.login-btn', timeout=10000)
                await login_button.click()
                await asyncio.sleep(2)
            except:
                print("âš ï¸ æœªæ‰¾åˆ°ç™»å½•æŒ‰é’®ï¼Œå¯èƒ½å·²ç»ç™»å½•")

            print("ğŸ“± è¯·ä½¿ç”¨å°çº¢ä¹¦APPæ‰«æäºŒç»´ç ç™»å½•...")
            print("â³ ç­‰å¾…ç™»å½•å®Œæˆ...")

            # ç­‰å¾…ç™»å½•æˆåŠŸï¼ˆæ£€æµ‹é¡µé¢å˜åŒ–ï¼‰
            try:
                await self.page.wait_for_selector('.user-info', timeout=60000)
                print("âœ… ç™»å½•æˆåŠŸï¼")

                # è·å–ç™»å½•åçš„ Cookie
                cookies = await self.browser_context.cookies()
                cookie_str = '; '.join([f"{cookie['name']}={cookie['value']}" for cookie in cookies])
                self.cookie = cookie_str
                self.session.headers['Cookie'] = cookie_str

                return True
            except:
                print("â° ç™»å½•è¶…æ—¶ï¼Œè¯·é‡è¯•")
                return False

        except Exception as e:
            print(f"âŒ ç™»å½•å¤±è´¥: {e}")
            return False

    async def close_browser(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.browser_context:
            await self.browser_context.close()
            print("âœ… æµè§ˆå™¨å·²å…³é—­")

async def async_main():
    """å¼‚æ­¥ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å°çº¢ä¹¦çœŸå®æ•°æ®çˆ¬è™« - åŸºäº MediaCrawler æ¶æ„')
    parser.add_argument('--keywords', type=str, default='ç¾å¦†,æŠ¤è‚¤,ç©¿æ­',
                       help='æœç´¢å…³é”®è¯ï¼Œç”¨é€—å·åˆ†éš”')
    parser.add_argument('--limit', type=int, default=10,
                       help='æ¯ä¸ªå…³é”®è¯è·å–çš„ç¬”è®°æ•°é‡')
    parser.add_argument('--cookie', type=str, default='',
                       help='å°çº¢ä¹¦Cookie')
    parser.add_argument('--playwright', action='store_true',
                       help='ä½¿ç”¨ Playwright æ¨¡å¼')
    parser.add_argument('--headless', action='store_true',
                       help='æ— å¤´æ¨¡å¼è¿è¡Œæµè§ˆå™¨')
    parser.add_argument('--login', action='store_true',
                       help='æ˜¯å¦éœ€è¦äºŒç»´ç ç™»å½•')

    args = parser.parse_args()

    print("ğŸš€ å¯åŠ¨å°çº¢ä¹¦æ•°æ®çˆ¬è™« - MediaCrawler æ¶æ„")
    print(f"ğŸ“ å…³é”®è¯: {args.keywords}")
    print(f"ğŸ“Š æ¯ä¸ªå…³é”®è¯é™åˆ¶: {args.limit} æ¡")
    print(f"ğŸ”§ æ¨¡å¼: {'Playwright' if args.playwright else 'Requests'}")
    print(f"ğŸ” ç™»å½•æ¨¡å¼: {'æ˜¯' if args.login else 'å¦'}")

    # ä»é…ç½®æ–‡ä»¶è¯»å– Cookie
    cookie = args.cookie
    if not cookie:
        try:
            with open('cookie_config.json', 'r', encoding='utf-8') as f:
                config = json.load(f)
                cookie = config.get('default', {}).get('cookie_string', '')
        except:
            pass

    crawler = MediaCrawlerXHS(cookie=cookie, use_playwright=args.playwright)

    try:
        # å¦‚æœä½¿ç”¨ Playwright æ¨¡å¼
        if args.playwright:
            browser_ok = await crawler.init_browser(headless=args.headless)

            # å¦‚æœéœ€è¦ç™»å½•
            if args.login and browser_ok:
                login_ok = await crawler.login_by_qrcode()
                if not login_ok:
                    print("âŒ ç™»å½•å¤±è´¥ï¼Œå°†ç»§ç»­ä½¿ç”¨ç°æœ‰Cookie")

        # å¤„ç†å…³é”®è¯
        keywords = [kw.strip() for kw in args.keywords.split(',')]
        all_notes = []

        # ä¸ºæ¯ä¸ªå…³é”®è¯è·å–æ•°æ®
        for keyword in keywords:
            print(f"\nğŸ” å¤„ç†å…³é”®è¯: {keyword}")
            notes = crawler.search_notes_by_keyword(keyword, limit=args.limit)
            all_notes.extend(notes)

            # éšæœºå»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(random.uniform(1, 3))

        # ä¿å­˜æ•°æ®
        print(f"\nğŸ’¾ ä¿å­˜æ•°æ®...")
        crawler.save_data(all_notes, 'real_xhs_notes_mediacrawler.json')

        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
        print(f"   æ€»ç¬”è®°æ•°: {len(all_notes)}")
        print(f"   æ¶‰åŠå…³é”®è¯: {', '.join(keywords)}")

        # ç»Ÿè®¡çœŸå®æ•°æ®å’Œæ¨¡æ‹Ÿæ•°æ®
        real_notes = [note for note in all_notes if not note['id'].startswith('mock_')]
        mock_notes = [note for note in all_notes if note['id'].startswith('mock_')]

        print(f"   - çœŸå®æ•°æ®: {len(real_notes)} æ¡")
        print(f"   - æ¨¡æ‹Ÿæ•°æ®: {len(mock_notes)} æ¡")

        print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼æ•°æ®å·²ä¿å­˜åˆ° data/ ç›®å½•")

    except Exception as e:
        print(f"âŒ çˆ¬å–è¿‡ç¨‹å‡ºé”™: {e}")
    finally:
        if args.playwright:
            await crawler.close_browser()

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨çœŸå®å°çº¢ä¹¦çˆ¬è™«...")

    # å°è¯•ä»é…ç½®æ–‡ä»¶è¯»å–Cookie
    cookie = ""
    cookie_file = os.path.join(os.path.dirname(__file__), 'cookie_config.json')
    if os.path.exists(cookie_file):
        try:
            with open(cookie_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
                cookie = config.get('cookie', '')
                print("âœ… ä»é…ç½®æ–‡ä»¶è¯»å–Cookie")
        except Exception as e:
            print(f"âš ï¸ è¯»å–Cookieé…ç½®å¤±è´¥: {e}")

    # åˆå§‹åŒ–çˆ¬è™«
    crawler = RealXhsCrawler(cookie=cookie)

    # è·å–çƒ­é—¨å…³é”®è¯
    keywords = crawler.get_trending_keywords()
    crawler.save_data(keywords, 'real_trending_keywords.json')

    # çˆ¬å–çƒ­é—¨è¯é¢˜
    all_notes = []
    for keyword_data in keywords[:3]:  # åªçˆ¬å–å‰3ä¸ªå…³é”®è¯
        keyword = keyword_data['keyword']
        print(f"\nğŸ“ æ­£åœ¨çˆ¬å–å…³é”®è¯: {keyword}")

        notes = crawler.search_notes(keyword, limit=7)

        # ä¸ºæ¯ä¸ªç¬”è®°è·å–è¯„è®ºï¼ˆä»…å‰2ä¸ªç¬”è®°ï¼‰
        for i, note in enumerate(notes[:2]):
            if note.get('id'):
                print(f"ğŸ’¬ è·å–ç¬”è®°è¯„è®º: {note['title'][:20]}...")
                comments = crawler.get_note_comments(note['id'], limit=5)
                note['comments'] = comments

        all_notes.extend(notes)
        time.sleep(2)  # å»¶è¿Ÿé¿å…è¢«é™åˆ¶

    crawler.save_data(all_notes, 'real_hot_topics.json')

    print(f"\nâœ… çˆ¬è™«å®Œæˆï¼è·å–äº† {len(all_notes)} æ¡ç¬”è®°æ•°æ®")

    # ç»Ÿè®¡ä¿¡æ¯
    real_notes = [note for note in all_notes if not note['id'].startswith('mock_')]
    mock_notes = [note for note in all_notes if note['id'].startswith('mock_')]

    print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:")
    print(f"   - çœŸå®æ•°æ®: {len(real_notes)} æ¡")
    print(f"   - æ¨¡æ‹Ÿæ•°æ®: {len(mock_notes)} æ¡")

    if cookie:
        print(f"ğŸ”‘ ä½¿ç”¨äº†Cookieè¿›è¡Œæ•°æ®è·å–")
    else:
        print(f"âš ï¸ æœªä½¿ç”¨Cookieï¼Œä¸»è¦ä¸ºæ¨¡æ‹Ÿæ•°æ®")

if __name__ == "__main__":
    # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚æ­¥å‚æ•°
    import sys
    if '--playwright' in sys.argv or '--login' in sys.argv:
        # ä½¿ç”¨å¼‚æ­¥æ¨¡å¼
        asyncio.run(async_main())
    else:
        # ä½¿ç”¨åŒæ­¥æ¨¡å¼ï¼ˆå…¼å®¹åŸæœ‰åŠŸèƒ½ï¼‰
        main()
