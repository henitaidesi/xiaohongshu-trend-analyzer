#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIåˆ†ææœåŠ¡ - æ™ºèƒ½æ•°æ®åˆ†æå’Œå¤„ç†
"""

import re
import json
import math
import random
from datetime import datetime, timedelta
from typing import List, Dict, Any, Tuple
from collections import Counter, defaultdict

class AIAnalysisService:
    def __init__(self):
        """åˆå§‹åŒ–AIåˆ†ææœåŠ¡"""
        # æƒ…æ„Ÿè¯å…¸
        self.positive_words = {
            'å¥½', 'æ£’', 'èµ', 'å–œæ¬¢', 'çˆ±', 'ç¾', 'æ¼‚äº®', 'å®Œç¾', 'ä¼˜ç§€', 'æ¨è',
            'å€¼å¾—', 'æ»¡æ„', 'æƒŠè‰³', 'å¿ƒåŠ¨', 'ç§è‰', 'å¿…ä¹°', 'ç¥ä»™', 'ç»äº†', 'çˆ±äº†',
            'å¤ªå¥½äº†', 'è¶…çº§', 'éå¸¸', 'ç‰¹åˆ«', 'çœŸçš„', 'ç¡®å®', 'æ•ˆæœå¥½', 'è´¨é‡å¥½'
        }
        
        self.negative_words = {
            'å·®', 'å', 'çƒ‚', 'éš¾ç”¨', 'ä¸å¥½', 'å¤±æœ›', 'åæ‚”', 'è¸©é›·', 'ä¸æ¨è',
            'æµªè´¹', 'éª—äºº', 'å‡çš„', 'åŠ£è´¨', 'ç³Ÿç³•', 'æ¶å¿ƒ', 'è®¨åŒ', 'åƒåœ¾',
            'ä¸å€¼', 'å‘', 'ç¿»è½¦', 'é¿é›·', 'æ…ä¹°', 'åˆ«ä¹°', 'é€€è´§', 'æŠ•è¯‰'
        }
        
        # åˆ†ç±»å…³é”®è¯
        self.category_keywords = {
            'ç¾å¦†': ['å£çº¢', 'ç²‰åº•', 'çœ¼å½±', 'ç«æ¯›è†', 'è…®çº¢', 'åŒ–å¦†', 'å½©å¦†', 'ç¾å¦†', 'æŠ¤è‚¤å“'],
            'æŠ¤è‚¤': ['é¢è†œ', 'ç²¾å', 'ä¹³æ¶²', 'æ´é¢', 'é˜²æ™’', 'æŠ¤è‚¤', 'ä¿å…»', 'æŠ—è€', 'è¡¥æ°´'],
            'ç©¿æ­': ['ç©¿æ­', 'æ­é…', 'è¡£æœ', 'è£™å­', 'å¤–å¥—', 'é‹å­', 'åŒ…åŒ…', 'æ—¶å°š', 'é£æ ¼'],
            'ç¾é£Ÿ': ['ç¾é£Ÿ', 'é£Ÿè°±', 'çƒ˜ç„™', 'é¤å…', 'å°åƒ', 'ç”œå“', 'æ–™ç†', 'åšé¥­', 'å¥½åƒ'],
            'æ—…è¡Œ': ['æ—…è¡Œ', 'æ—…æ¸¸', 'æ”»ç•¥', 'æ™¯ç‚¹', 'é…’åº—', 'æœºç¥¨', 'ç­¾è¯', 'æ¸¸è®°', 'æ‰“å¡'],
            'å¥èº«': ['å¥èº«', 'è¿åŠ¨', 'ç‘œä¼½', 'å‡è‚¥', 'å¡‘å½¢', 'é”»ç‚¼', 'è·‘æ­¥', 'å¥åº·', 'ä½“é‡'],
            'æ•°ç ': ['æ‰‹æœº', 'ç”µè„‘', 'ç›¸æœº', 'è€³æœº', 'æ•°ç ', 'ç§‘æŠ€', 'æµ‹è¯„', 'å¼€ç®±', 'é…ç½®'],
            'å®¶å±…': ['å®¶å±…', 'è£…ä¿®', 'æ”¶çº³', 'å®¶å…·', 'è£…é¥°', 'æ¸…æ´', 'æ•´ç†', 'å¸ƒç½®', 'è®¾è®¡']
        }
        
    def analyze_sentiment(self, text: str) -> Dict[str, Any]:
        """æƒ…æ„Ÿåˆ†æ"""
        if not text:
            return {'sentiment': 'neutral', 'score': 0.5, 'confidence': 0.0}
        
        text = text.lower()
        
        # è®¡ç®—æ­£è´Ÿé¢è¯æ±‡æ•°é‡
        positive_count = sum(1 for word in self.positive_words if word in text)
        negative_count = sum(1 for word in self.negative_words if word in text)
        
        # è®¡ç®—æƒ…æ„Ÿåˆ†æ•°
        total_words = len(text.split())
        if total_words == 0:
            return {'sentiment': 'neutral', 'score': 0.5, 'confidence': 0.0}
        
        positive_ratio = positive_count / total_words
        negative_ratio = negative_count / total_words
        
        # ç¡®å®šæƒ…æ„Ÿå€¾å‘
        if positive_ratio > negative_ratio:
            sentiment = 'positive'
            score = 0.5 + min(0.5, positive_ratio * 10)
        elif negative_ratio > positive_ratio:
            sentiment = 'negative'
            score = 0.5 - min(0.5, negative_ratio * 10)
        else:
            sentiment = 'neutral'
            score = 0.5
        
        # è®¡ç®—ç½®ä¿¡åº¦
        confidence = min(1.0, (positive_count + negative_count) / max(1, total_words) * 5)
        
        return {
            'sentiment': sentiment,
            'score': score,
            'confidence': confidence,
            'positive_words': positive_count,
            'negative_words': negative_count
        }
    
    def classify_content(self, text: str) -> Dict[str, Any]:
        """å†…å®¹åˆ†ç±»"""
        if not text:
            return {'category': 'å…¶ä»–', 'confidence': 0.0, 'keywords': []}
        
        text = text.lower()
        category_scores = {}
        matched_keywords = {}
        
        # è®¡ç®—æ¯ä¸ªåˆ†ç±»çš„åŒ¹é…åˆ†æ•°
        for category, keywords in self.category_keywords.items():
            score = 0
            matched = []
            for keyword in keywords:
                if keyword in text:
                    score += 1
                    matched.append(keyword)
            
            if score > 0:
                category_scores[category] = score
                matched_keywords[category] = matched
        
        # ç¡®å®šæœ€ä½³åˆ†ç±»
        if not category_scores:
            return {'category': 'å…¶ä»–', 'confidence': 0.0, 'keywords': []}
        
        best_category = max(category_scores.items(), key=lambda x: x[1])
        confidence = min(1.0, best_category[1] / 3)  # æœ€å¤š3ä¸ªå…³é”®è¯åŒ¹é…ä¸ºæ»¡åˆ†
        
        return {
            'category': best_category[0],
            'confidence': confidence,
            'keywords': matched_keywords.get(best_category[0], []),
            'all_scores': category_scores
        }
    
    def calculate_engagement_score(self, data: Dict[str, Any]) -> float:
        """è®¡ç®—å‚ä¸åº¦åˆ†æ•°"""
        like_count = data.get('like_count', 0)
        comment_count = data.get('comment_count', 0)
        share_count = data.get('share_count', 0)
        view_count = data.get('view_count', 0)
        
        # åŠ æƒè®¡ç®—å‚ä¸åº¦
        engagement = (
            like_count * 1.0 +
            comment_count * 2.0 +  # è¯„è®ºæƒé‡æ›´é«˜
            share_count * 3.0 +    # åˆ†äº«æƒé‡æœ€é«˜
            view_count * 0.1       # æµè§ˆé‡æƒé‡è¾ƒä½
        )
        
        # å½’ä¸€åŒ–åˆ°0-100åˆ†
        score = min(100, math.log10(max(1, engagement)) * 20)
        
        return round(score, 2)
    
    def extract_keywords(self, texts: List[str], top_k: int = 20) -> List[Dict[str, Any]]:
        """æå–å…³é”®è¯"""
        if not texts:
            return []
        
        # ç®€å•çš„è¯é¢‘ç»Ÿè®¡
        word_counts = Counter()
        
        for text in texts:
            if not text:
                continue
            
            # ç®€å•çš„ä¸­æ–‡åˆ†è¯ï¼ˆåŸºäºå¸¸è§åˆ†éš”ç¬¦ï¼‰
            words = re.findall(r'[\u4e00-\u9fff]+', text)
            
            # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
            stop_words = {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ'}
            filtered_words = [word for word in words if len(word) >= 2 and word not in stop_words]
            
            word_counts.update(filtered_words)
        
        # è½¬æ¢ä¸ºç»“æœæ ¼å¼
        keywords = []
        for word, count in word_counts.most_common(top_k):
            keywords.append({
                'keyword': word,
                'frequency': count,
                'weight': round(count / max(1, len(texts)), 3)
            })
        
        return keywords
    
    def analyze_user_behavior(self, notes: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”¨æˆ·è¡Œä¸ºåˆ†æ"""
        if not notes:
            return {}
        
        # ç»Ÿè®¡ç”¨æˆ·æ´»è·ƒæ—¶é—´
        hour_distribution = defaultdict(int)
        category_preferences = defaultdict(int)
        engagement_levels = {'high': 0, 'medium': 0, 'low': 0}
        sentiment_distribution = {'positive': 0, 'negative': 0, 'neutral': 0}
        
        for note in notes:
            # æ—¶é—´åˆ†æ
            try:
                publish_time = datetime.fromisoformat(note.get('publish_time', ''))
                hour_distribution[publish_time.hour] += 1
            except:
                pass
            
            # åˆ†ç±»åå¥½
            category = note.get('category', 'å…¶ä»–')
            category_preferences[category] += 1
            
            # å‚ä¸åº¦åˆ†æ
            engagement = self.calculate_engagement_score(note)
            if engagement >= 70:
                engagement_levels['high'] += 1
            elif engagement >= 40:
                engagement_levels['medium'] += 1
            else:
                engagement_levels['low'] += 1
            
            # æƒ…æ„Ÿåˆ†æ
            content = note.get('content', '') + ' ' + note.get('title', '')
            sentiment = self.analyze_sentiment(content)
            sentiment_distribution[sentiment['sentiment']] += 1
        
        # æ‰¾å‡ºæœ€æ´»è·ƒæ—¶é—´æ®µ
        peak_hours = sorted(hour_distribution.items(), key=lambda x: x[1], reverse=True)[:3]
        
        return {
            'total_notes': len(notes),
            'peak_hours': [{'hour': h, 'count': c} for h, c in peak_hours],
            'category_preferences': dict(category_preferences),
            'engagement_levels': dict(engagement_levels),
            'sentiment_distribution': dict(sentiment_distribution),
            'avg_engagement': round(sum(self.calculate_engagement_score(note) for note in notes) / len(notes), 2)
        }
    
    def predict_trend(self, historical_data: List[Dict[str, Any]], days_ahead: int = 7) -> Dict[str, Any]:
        """è¶‹åŠ¿é¢„æµ‹"""
        if not historical_data:
            return {}
        
        # æŒ‰æ—¶é—´æ’åº
        sorted_data = sorted(historical_data, key=lambda x: x.get('publish_time', ''))
        
        # ç®€å•çš„çº¿æ€§è¶‹åŠ¿é¢„æµ‹
        daily_stats = defaultdict(lambda: {'count': 0, 'engagement': 0})
        
        for item in sorted_data:
            try:
                date = datetime.fromisoformat(item.get('publish_time', '')).date()
                daily_stats[date]['count'] += 1
                daily_stats[date]['engagement'] += self.calculate_engagement_score(item)
            except:
                continue
        
        # è®¡ç®—å¹³å‡å€¼
        for date, stats in daily_stats.items():
            if stats['count'] > 0:
                stats['avg_engagement'] = stats['engagement'] / stats['count']
        
        # é¢„æµ‹æœªæ¥è¶‹åŠ¿
        if len(daily_stats) < 2:
            return {'prediction': 'insufficient_data'}
        
        recent_days = list(sorted(daily_stats.keys()))[-7:]  # æœ€è¿‘7å¤©
        
        # è®¡ç®—å¢é•¿ç‡
        if len(recent_days) >= 2:
            early_avg = sum(daily_stats[day]['count'] for day in recent_days[:3]) / 3
            late_avg = sum(daily_stats[day]['count'] for day in recent_days[-3:]) / 3
            
            growth_rate = (late_avg - early_avg) / max(1, early_avg)
            
            if growth_rate > 0.1:
                trend = 'increasing'
            elif growth_rate < -0.1:
                trend = 'decreasing'
            else:
                trend = 'stable'
        else:
            trend = 'stable'
            growth_rate = 0
        
        return {
            'trend': trend,
            'growth_rate': round(growth_rate * 100, 2),
            'confidence': min(1.0, len(recent_days) / 7),
            'prediction_days': days_ahead,
            'daily_stats': {str(k): v for k, v in daily_stats.items()}
        }
    
    def generate_insights(self, notes: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆæ™ºèƒ½æ´å¯Ÿ"""
        if not notes:
            return {}
        
        # ç»¼åˆåˆ†æ
        user_behavior = self.analyze_user_behavior(notes)
        trend_prediction = self.predict_trend(notes)
        
        # æå–å†…å®¹å…³é”®è¯
        all_content = [note.get('content', '') + ' ' + note.get('title', '') for note in notes]
        keywords = self.extract_keywords(all_content, top_k=10)
        
        # ç”Ÿæˆæ´å¯Ÿ
        insights = []
        
        # åˆ†ç±»æ´å¯Ÿ
        if user_behavior.get('category_preferences'):
            top_category = max(user_behavior['category_preferences'].items(), key=lambda x: x[1])
            insights.append(f"æœ€å—æ¬¢è¿çš„å†…å®¹ç±»åˆ«æ˜¯{top_category[0]}ï¼Œå æ¯”{round(top_category[1]/len(notes)*100, 1)}%")
        
        # å‚ä¸åº¦æ´å¯Ÿ
        if user_behavior.get('engagement_levels'):
            high_engagement = user_behavior['engagement_levels'].get('high', 0)
            if high_engagement > len(notes) * 0.3:
                insights.append(f"å†…å®¹è´¨é‡è¾ƒé«˜ï¼Œ{round(high_engagement/len(notes)*100, 1)}%çš„å†…å®¹è·å¾—äº†é«˜å‚ä¸åº¦")
        
        # æƒ…æ„Ÿæ´å¯Ÿ
        if user_behavior.get('sentiment_distribution'):
            positive_ratio = user_behavior['sentiment_distribution'].get('positive', 0) / len(notes)
            if positive_ratio > 0.6:
                insights.append(f"ç”¨æˆ·åé¦ˆç§¯æï¼Œ{round(positive_ratio*100, 1)}%çš„å†…å®¹æƒ…æ„Ÿå€¾å‘ä¸ºæ­£é¢")
        
        # è¶‹åŠ¿æ´å¯Ÿ
        if trend_prediction.get('trend'):
            trend = trend_prediction['trend']
            if trend == 'increasing':
                insights.append(f"å†…å®¹çƒ­åº¦å‘ˆä¸Šå‡è¶‹åŠ¿ï¼Œå¢é•¿ç‡ä¸º{trend_prediction.get('growth_rate', 0)}%")
            elif trend == 'decreasing':
                insights.append(f"å†…å®¹çƒ­åº¦å‘ˆä¸‹é™è¶‹åŠ¿ï¼Œéœ€è¦å…³æ³¨å†…å®¹è´¨é‡")
        
        return {
            'insights': insights,
            'keywords': keywords,
            'user_behavior': user_behavior,
            'trend_prediction': trend_prediction,
            'analysis_time': datetime.now().isoformat()
        }

# å…¨å±€AIåˆ†ææœåŠ¡å®ä¾‹
ai_service = AIAnalysisService()

def main():
    """æµ‹è¯•AIåˆ†ææœåŠ¡"""
    print("ğŸ¤– æµ‹è¯•AIåˆ†ææœåŠ¡...")
    
    # æµ‹è¯•æ•°æ®
    test_notes = [
        {
            'title': 'è¿™æ¬¾å£çº¢çœŸçš„å¤ªå¥½çœ‹äº†ï¼',
            'content': 'é¢œè‰²è¶…çº§ç¾ï¼Œè´¨åœ°ä¹Ÿå¾ˆå¥½ï¼Œå¼ºçƒˆæ¨èç»™å¤§å®¶ï¼',
            'category': 'ç¾å¦†',
            'like_count': 150,
            'comment_count': 20,
            'share_count': 5,
            'view_count': 1000,
            'publish_time': datetime.now().isoformat()
        },
        {
            'title': 'ç©¿æ­åˆ†äº«',
            'content': 'ä»Šå¤©çš„æ­é…å¾ˆæ»¡æ„ï¼Œç®€çº¦åˆæ—¶å°š',
            'category': 'ç©¿æ­',
            'like_count': 80,
            'comment_count': 10,
            'share_count': 2,
            'view_count': 500,
            'publish_time': (datetime.now() - timedelta(hours=2)).isoformat()
        }
    ]
    
    # æµ‹è¯•å„é¡¹åŠŸèƒ½
    print("\nğŸ“Š æƒ…æ„Ÿåˆ†ææµ‹è¯•:")
    sentiment = ai_service.analyze_sentiment("è¿™æ¬¾äº§å“çœŸçš„å¤ªå¥½ç”¨äº†ï¼Œå¼ºçƒˆæ¨èï¼")
    print(f"   ç»“æœ: {sentiment}")
    
    print("\nğŸ·ï¸ å†…å®¹åˆ†ç±»æµ‹è¯•:")
    classification = ai_service.classify_content("è¿™æ¬¾å£çº¢é¢œè‰²å¾ˆç¾ï¼Œè´¨åœ°ä¹Ÿä¸é”™")
    print(f"   ç»“æœ: {classification}")
    
    print("\nğŸ“ˆ å‚ä¸åº¦åˆ†ææµ‹è¯•:")
    engagement = ai_service.calculate_engagement_score(test_notes[0])
    print(f"   å‚ä¸åº¦åˆ†æ•°: {engagement}")
    
    print("\nğŸ” å…³é”®è¯æå–æµ‹è¯•:")
    keywords = ai_service.extract_keywords([note['content'] for note in test_notes])
    print(f"   å…³é”®è¯: {keywords[:5]}")
    
    print("\nğŸ§  æ™ºèƒ½æ´å¯Ÿæµ‹è¯•:")
    insights = ai_service.generate_insights(test_notes)
    print(f"   æ´å¯Ÿ: {insights['insights']}")
    
    print("\nâœ… AIåˆ†ææœåŠ¡æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    main()
